{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd8979df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import re\n",
    "\n",
    "# Set device to CPU\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Load data from JSON files\n",
    "train_data = json.load(open('train279_2a.json', encoding=\"utf8\"))\n",
    "valid_data = json.load(open('validation.json', encoding=\"utf8\"))\n",
    "test_data = json.load(open('dev_unlabeled.json', encoding=\"utf8\"))\n",
    "\n",
    "# Define image data paths\n",
    "IMAGE_DATA_TRAIN = 'train_images/'\n",
    "IMAGE_DATA_VALID = 'validation_images/'\n",
    "\n",
    "# Define label names\n",
    "LABELS = [\n",
    "    'Causal Oversimplification',\n",
    "    'Transfer',\n",
    "    'Flag-waving',\n",
    "    'Black-and-white Fallacy/Dictatorship',\n",
    "    'Smears',\n",
    "    'Loaded Language',\n",
    "    'Glittering generalities (Virtue)',\n",
    "    'Thought-terminating cliché',\n",
    "    'Whataboutism',\n",
    "    'Slogans',\n",
    "    'Doubt',\n",
    "    'Name calling/Labeling',\n",
    "    'Repetition',\n",
    "    'Appeal to authority',\n",
    "    'Appeal to (Strong) Emotions',\n",
    "    'Reductio ad hitlerum',\n",
    "    'Appeal to fear/prejudice',\n",
    "    'Exaggeration/Minimisation',\n",
    "    'Misrepresentation of Someone\\'s Position (Straw Man)',\n",
    "    'Obfuscation, Intentional vagueness, Confusion',\n",
    "    'Bandwagon',\n",
    "    'Presenting Irrelevant Data (Red Herring)'\n",
    "]\n",
    "\n",
    "# Define dataset class\n",
    "class ModelDataSet(Dataset):\n",
    "    def __init__(self, tokenizer, max_length, data, image_path):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.image_path = image_path\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.image_path + self.data[index]['image']\n",
    "        img = Image.open(image_path).convert('RGB')  # Ensure image is RGB\n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "        img_tensor = preprocess(img)\n",
    "        \n",
    "        text = self.data[index]['text']\n",
    "        text = clean_text(text)\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            pad_to_max_length=True,\n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "            max_length=self.max_length,\n",
    "        )\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'image': img_tensor,\n",
    "        }\n",
    "\n",
    "# Define BERT tokenizer\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Create dataset instances\n",
    "train_dataset = ModelDataSet(tokenizer, max_length=512, data=train_data, image_path=IMAGE_DATA_TRAIN)\n",
    "valid_dataset = ModelDataSet(tokenizer, max_length=512, data=valid_data, image_path=IMAGE_DATA_VALID)\n",
    "\n",
    "# Define dataloaders\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=4, shuffle=True)\n",
    "valid_dataloader = DataLoader(dataset=valid_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# Define the model architecture\n",
    "class MultiModalClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MultiModalClassifier, self).__init__()\n",
    "        self.bert_model = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.resnet_model = models.resnet50(pretrained=True)\n",
    "        self.resnet_model.fc = nn.Identity()  # Remove the final classification layer of ResNet\n",
    "\n",
    "        self.txt_dense1 = nn.Linear(768, 256)\n",
    "        self.img_dense1 = nn.Linear(2048, 256)\n",
    "        self.concat_dense2 = nn.Linear(256, num_classes)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.bn2 = nn.BatchNorm1d(num_classes)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids, img_input):\n",
    "        _, bert_output = self.bert_model(input_ids=ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n",
    "        txt_repr = self.dropout(F.relu(self.bn1(self.txt_dense1(bert_output))))\n",
    "\n",
    "        img_repr = self.resnet_model(img_input)\n",
    "        img_repr = self.dropout(F.relu(self.bn1(self.img_dense1(img_repr))))\n",
    "        \n",
    "        combined_repr = torch.cat((txt_repr, img_repr), dim=1)\n",
    "        combined_repr = self.dropout(F.relu(self.concat_dense2(combined_repr)))\n",
    "        combined_repr = self.bn1(combined_repr) \n",
    "        return F.softmax(self.bn2(combined_repr), dim=1)\n",
    "\n",
    "# Create model instance\n",
    "model = MultiModalClassifier(num_classes=len(LABELS))\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1eb637ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/730 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 513.932098031044, Accuracy: 0.0205620287868403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 512.5630480051041, Accuracy: 0.021247429746401644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 512.5664746761322, Accuracy: 0.02227553118574366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 512.5655576586723, Accuracy: 0.02227553118574366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 512.5859272480011, Accuracy: 0.02193283070596299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 512.5642596483231, Accuracy: 0.02193283070596299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 512.5697718858719, Accuracy: 0.02193283070596299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/10], Loss: 512.5677834153175, Accuracy: 0.02227553118574366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/10], Loss: 512.5979398488998, Accuracy: 0.02227553118574366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Loss: 512.5607372522354, Accuracy: 0.02227553118574366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 512.5647913217545, Accuracy: 0.022618231665524333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 213\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 213\u001b[0m train_model(\u001b[38;5;241m10\u001b[39m, train_dataloader, model, loss_fn, optimizer)\n\u001b[0;32m    215\u001b[0m \u001b[38;5;66;03m# Evaluate the model on validation set\u001b[39;00m\n\u001b[0;32m    216\u001b[0m evaluate_model(valid_dataloader, model)\n",
      "Cell \u001b[1;32mIn[10], line 173\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(epochs, dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m    171\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(ids, mask, token_type_ids, images)\n\u001b[0;32m    172\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, labels)\n\u001b[1;32m--> 173\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    174\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    176\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    524\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    267\u001b[0m     tensors,\n\u001b[0;32m    268\u001b[0m     grad_tensors_,\n\u001b[0;32m    269\u001b[0m     retain_graph,\n\u001b[0;32m    270\u001b[0m     create_graph,\n\u001b[0;32m    271\u001b[0m     inputs,\n\u001b[0;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    274\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import re\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Specify data paths and other configurations\n",
    "IMAGE_DATA_TRAIN = 'train_images/train_images/'\n",
    "IMAGE_DATA_VALID = 'validation_images/validation_images/'\n",
    "JSON_DATA_TRAIN = 'train279_2a.json'\n",
    "JSON_DATA_VALID = 'validation.json'\n",
    "JSON_DATA_TEST = 'dev_unlabeled.json'\n",
    "\n",
    "# Define labels\n",
    "LABELS = {\n",
    "    'Appeal to (Strong) Emotions': 0,\n",
    "    'Appeal to authority': 1,\n",
    "    'Appeal to fear/prejudice': 2,\n",
    "    'Bandwagon': 3,\n",
    "    'Black-and-white Fallacy/Dictatorship': 4,\n",
    "    'Causal Oversimplification': 5,\n",
    "    'Doubt': 6,\n",
    "    'Exaggeration/Minimisation': 7,\n",
    "    'Flag-waving': 8,\n",
    "    'Glittering generalities (Virtue)': 9,\n",
    "    'Loaded Language': 10,\n",
    "    \"Misrepresentation of Someone's Position (Straw Man)\": 11,\n",
    "    'Name calling/Labeling': 12,\n",
    "    'Obfuscation, Intentional vagueness, Confusion': 13,\n",
    "    'Presenting Irrelevant Data (Red Herring)': 14,\n",
    "    'Reductio ad hitlerum': 15,\n",
    "    'Repetition': 16,\n",
    "    'Slogans': 17,\n",
    "    'Whataboutism': 18,\n",
    "    'Thought-terminating cliché': 19,\n",
    "    'Transfer': 20,\n",
    "    'Smears': 21\n",
    "}\n",
    "\n",
    "# Load data from JSON files\n",
    "train_data = json.load(open(JSON_DATA_TRAIN, encoding=\"utf8\"))\n",
    "valid_data = json.load(open(JSON_DATA_VALID, encoding=\"utf8\"))\n",
    "test_data = json.load(open(JSON_DATA_TEST, encoding=\"utf8\"))\n",
    "\n",
    "# Data Preprocessing\n",
    "def clean_text(text):\n",
    "    text = text.replace('\\\\n', ' ').lower()\n",
    "    text = re.sub(r'[^\\\\w\\\\s]', \"\", text)\n",
    "    text = re.sub(r'\\\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Define Dataset class\n",
    "class ModelDataSet(Dataset):\n",
    "    def __init__(self, tokenizer, max_length, data, image_path):\n",
    "        super(ModelDataSet, self).__init__()\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.image_path = image_path\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_name = self.data[index]['image']\n",
    "        img = Image.open(self.image_path + image_name)\n",
    "        preprocess = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        img_tensor = preprocess(img)\n",
    "        if img_tensor.shape[0] > 3 :\n",
    "            img_tensor = img_tensor[:3, :, :]\n",
    "        elif img_tensor.shape[0] < 3:\n",
    "            img_tensor= img_tensor.expand(3, -1, -1)\n",
    "        img_tensor = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(img_tensor)\n",
    "        \n",
    "        text = clean_text(self.data[index]['text'])\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            pad_to_max_length=True,\n",
    "            add_special_tokens=True,\n",
    "            return_attention_mask=True,\n",
    "            max_length=self.max_length,\n",
    "        )\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "        target = torch.zeros(len(LABELS), dtype=torch.float)\n",
    "        for label in self.data[index]['labels']:\n",
    "            target[LABELS[label]] = 1.0\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            'image': img_tensor,\n",
    "            'target': target\n",
    "        }\n",
    "\n",
    "# Define model architecture\n",
    "class MultiModalClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, roberta_model_name=\"roberta-base\"):\n",
    "        super(MultiModalClassifier, self).__init__()\n",
    "        self.bert_model = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.resnet_model = models.resnet50(pretrained=True)\n",
    "        self.resnet_model.fc = nn.Identity()  # Remove the final classification layer of ResNet\n",
    "\n",
    "        self.txt_dense1 = nn.Linear(768, 256)\n",
    "        self.img_dense1 = nn.Linear(2048, 256)\n",
    "        self.concat_dense1 = nn.Linear(512, 512)  # Adjust the output dimension of the concatenation\n",
    "        self.concat_dense2 = nn.Linear(512, num_classes)\n",
    "        self.dropout = nn.Dropout(p=0.4)\n",
    "        self.bn1 = nn.BatchNorm1d(512)  # Adjust batch normalization dimension\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids, img_input):\n",
    "        _, bert_output = self.bert_model(input_ids=ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n",
    "        txt_repr = self.dropout(F.relu(self.txt_dense1(bert_output)))\n",
    "\n",
    "        img_repr = self.resnet_model(img_input)\n",
    "        img_repr = self.dropout(F.relu(self.img_dense1(img_repr)))\n",
    "\n",
    "        combined_repr = torch.cat((txt_repr, img_repr), dim=1)\n",
    "        combined_repr = self.dropout(F.relu(self.concat_dense1(combined_repr)))  # Apply dense layer before batch normalization\n",
    "        combined_repr = self.bn1(combined_repr)  # Apply batch normalization after adjusting dimension\n",
    "        return F.softmax(self.concat_dense2(combined_repr), dim=1)\n",
    "\n",
    "# Create DataLoader instances\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "train_dataset = ModelDataSet(tokenizer, max_length=512, data=train_data, image_path=IMAGE_DATA_TRAIN)\n",
    "valid_dataset = ModelDataSet(tokenizer, max_length=512, data=valid_data, image_path=IMAGE_DATA_VALID)\n",
    "train_dataloader = DataLoader(dataset=train_dataset, batch_size=4, shuffle=True)\n",
    "valid_dataloader = DataLoader(dataset=valid_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = MultiModalClassifier(num_classes=len(LABELS))\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001) \n",
    "\n",
    "# Training function\n",
    "def train_model(epochs, dataloader, model, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total_samples = 0\n",
    "        for batch in tqdm(dataloader, leave=False):\n",
    "            ids = batch['ids']\n",
    "            token_type_ids = batch['token_type_ids']\n",
    "            mask = batch['mask']\n",
    "            labels = batch['target']\n",
    "            images = batch['image']\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(ids, mask, token_type_ids, images)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            # Convert predicted tensor to have the same shape as labels tensor\n",
    "            predicted_one_hot = F.one_hot(predicted, num_classes=labels.size(1))\n",
    "            correct += (predicted_one_hot == labels).all(dim=1).sum().item()  # Compare one-hot tensors\n",
    "\n",
    "        accuracy = correct / total_samples\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {total_loss}, Accuracy: {accuracy}')\n",
    "\n",
    "# Train the model\n",
    "train_model(10, train_dataloader, model, loss_fn, optimizer)\n",
    "\n",
    "\n",
    "# Evaluate function\n",
    "def evaluate_model(dataloader, model):\n",
    "    model.eval()\n",
    "    total_samples = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, leave=False):\n",
    "            ids = batch['ids']\n",
    "            token_type_ids = batch['token_type_ids']\n",
    "            mask = batch['mask']\n",
    "            labels = batch['target']\n",
    "            images = batch['image']\n",
    "            \n",
    "            outputs = model(ids, mask, token_type_ids, images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = correct / total_samples\n",
    "    print(f'Accuracy: {accuracy}')\n",
    "\n",
    "# Train the model\n",
    "train_model(10, train_dataloader, model, loss_fn, optimizer)\n",
    "\n",
    "# Evaluate the model on validation set\n",
    "evaluate_model(valid_dataloader, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05ad63cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interrupt the kernel in Jupyter Notebook to stop the training\n",
    "\n",
    "# Save the model's state dictionary\n",
    "torch.save(model.state_dict(), 'model_checkpoint.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82bf02dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchsummary\n",
      "  Obtaining dependency information for torchsummary from https://files.pythonhosted.org/packages/7d/18/1474d06f721b86e6a9b9d7392ad68bed711a02f3b61ac43f13c719db50a6/torchsummary-1.5.1-py3-none-any.whl.metadata\n",
      "  Downloading torchsummary-1.5.1-py3-none-any.whl.metadata (296 bytes)\n",
      "Downloading torchsummary-1.5.1-py3-none-any.whl (2.8 kB)\n",
      "Installing collected packages: torchsummary\n",
      "Successfully installed torchsummary-1.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchsummary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1637a3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(dataloader, model):\n",
    "    model.eval()\n",
    "    total_samples = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, leave=False):\n",
    "            ids = batch['ids']\n",
    "            token_type_ids = batch['token_type_ids']\n",
    "            mask = batch['mask']\n",
    "            labels = batch['target']\n",
    "            images = batch['image']\n",
    "            \n",
    "            outputs = model(ids, mask, token_type_ids, images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            # Convert one-hot encoded labels to class indices\n",
    "            _, labels_idx = torch.max(labels, 1)\n",
    "            \n",
    "            total_samples += labels.size(0)\n",
    "            correct += (predicted == labels_idx).sum().item()\n",
    "    \n",
    "    accuracy = correct / total_samples\n",
    "    print(f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7d87e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "evaluate_model(valid_dataloader, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
